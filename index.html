<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0-beta3/css/all.min.css" rel="stylesheet">
    <link rel="stylesheet" href="./css/academicons.min.css" />
    <script>
        window.MathJax = {
            tex: {
                inlineMath: [['$', '$'], ['\\(', '\\)']],
                displayMath: [['$$', '$$'], ['\\[', '\\]']]
            },
            svg: {
                fontCache: 'global'
            }
        };
    </script>
    <script type="text/javascript" id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-svg.js"></script>
    <style>
        body {
            font-family: "Avenir Book", "HelveticaNeue-Light", "Helvetica Neue Light", "Helvetica Neue", Helvetica, Arial, "Lucida Grande", sans-serif;
            font-weight: 300;
            font-size: 14px;
            margin-left: auto;
            margin-right: auto;
            max-width: 65%;
            padding: 0 15px;
        }

        .header-logo {
            height: 50px;
        }

        .flex-container {
            display: flex;
            justify-content: space-between;
            gap: 20px;
            flex-wrap: wrap;
        }

        .flex-item {
            flex: 1;
            min-width: 200px;
        }

        .code-title {
            text-align: center;
            margin: 10px 0;
        }

        pre {
            margin: 0;
            white-space: pre-wrap;
        }

        code {
            display: block;
        }

        .header {
            width: 85%;
            text-align: right;
            padding: 10px 0;
        }

        .install,
        .code-container {
            background-color: #f8f9fa;
            padding: 15px;
            border-radius: 8px;
            text-align: left;
        }

        .disclaimerbox {
            background-color: #eee;
            border: 1px solid #eeeeee;
            border-radius: 10px;
            padding: 20px;
        }

        .link-pill {
            background-color: #000000;
            color: white !important;
            text-decoration: none;
            padding: 10px 20px;
            border-radius: 20px;
            font-size: 15px;
            display: inline-flex;
            align-items: center;
            margin: 5px;
            transition: background-color 0.3s, color 0.3s;
        }

        .link-pill:hover {
            background-color: #150E12;
            color: #FF0000 !important;
        }

        .link-pill i,
        .link-pill img {
            margin-right: 10px;
            color: white;
        }

        video.header-vid,
        img.header-img {
            height: 140px;
            border: 1px solid black;
            border-radius: 10px;
        }

        img.rounded {
            border: 0;
            border-radius: 10px;
        }

        a:link,
        a:visited {
            color: #1367a7;
            text-decoration: none;
        }

        a:hover {
            color: #208799;
        }

        hr {
            border: 0;
            height: 1px;
            background-image: linear-gradient(to right, rgba(0, 0, 0, 0), rgba(0, 0, 0, 0.75), rgba(0, 0, 0, 0));
        }

        table {
            width: 100%;
            max-width: 700px;
            margin: auto;
        }

        td {
            padding: 10px;
        }

        img {
            max-width: 100%;
            height: auto;
        }

        .install pre {
            margin: 0;
        }

        .code-container {
            margin-top: 10px;
        }
    </style>
    <title>FastTRAC</title>
</head>

<body>
    <div class="header">
        <img src="assets/seas_logo.png" alt="Logo" class="header-logo">
    </div>
    <br>
    <center>
        <span style="font-size:32px"><strong>Fast </strong>
            <color-profile><strong>TRAC</strong></color-profile>
            <strong> &nbsp;üèé</strong><br>
            <span style="font-size: 28px;">A Parameter-free Optimizer for <br> Lifelong Reinforcement Learning</span>
        </span><br><br><br>
    </center>

    <table align="center">
        <tbody>
            <tr>
                <td align="center">
                    <center>
                        <span style="font-size:20px"><a href="http://minyounghuh.com">Aneesh </a><a href="https://scholar.google.com/citations?user=OAWT85oAAAAJ&hl=en">Muppid</a><a>i</a><sup>1, 2</sup></span>
                    </center>
                </td>
                <td align="center">
                    <center>
                        <span style="font-size:20px"><a href="https://zhiyuzz.github.io">Zhiyu Zhang</a><sup>2</sup></span>
                    </center>
                </td>
                <td align="center">
                    <center>
                        <span style="font-size:20px"><a href="https://hankyang.seas.harvard.edu">Heng Yang</a><sup>2&nbsp;</sup></span>
                    </center>
                </td>
            </tr>
        </tbody>
    </table><br>

    <table align="center">
        <tbody>
            <tr>
                <td align="center">
                    <center>
                        <span style="font-size:18px"><sup>1</sup>Harvard College</span>
                    </center>
                </td>
                <td align="center">
                    <center>
                        <span style="font-size:18px"><sup>2</sup>Harvard SEAS</span>
                    </center>
                </td>
            </tr>
        </tbody>
    </table>
    <table align="center">
        <tbody>
            <tr>
                <td align="center">
                    <center>
                        <br>
                        <a href="https://arxiv.org/abs/2405.16642" target="_blank" class="link-pill">
                            <i style="padding-right:2px; font-size:23px" class="ai ai-arxiv" aria-hidden="true"></i> ArXiv
                        </a>
                        <br>
                    </center>
                </td>

                <td align="center">
                    <center>
                        <br>
                        <a href="https://github.com/ComputationalRobotics/PACE?tab=readme-ov-file" target="_blank" class="link-pill">
                            <i style="padding-right:2px;font-size:23px" class="fab fa-github" aria-hidden="true"></i>
                            Experiments

                        </a>
                        <br>
                    </center>
                </td>

                <td align="center">
                    <center>
                        <br>
                        <a href="https://colab.research.google.com/drive/1c5OxMa5fiSVnl5w6J7flrjNUteUkp6BV?usp=sharing" target="_blank" class="link-pill">
                            <img style="padding-bottom:3px;padding-right:2px;height:18px" src="assets/colab.png" aria-hidden="true">
                            Colab

                        </a>
                        <br>
                    </center>
                </td>
                <td align="center">
                    <center>
                        <br>
                        <a href="https://pypi.org/project/trac-optimizer/" target="_blank" class="link-pill">
                            <img style="padding-bottom:3px;padding-right:2px;height:18px" src="assets/python.png" aria-hidden="true">
                            Pypi

                        </a>
                        <br>
                    </center>
                </td>
            </tr>
        </tbody>
    </table>
    <br />
    <hr>
    <center>
        <h2>
            <strong> Abstract </strong>
        </h2>
    </center>
    <p>
        A key challenge in lifelong reinforcement learning (RL) is the loss of plasticity, where previous learning progress hinders an agent's adaptation to new tasks. While regularization and resetting can help, they require precise hyperparameter selection at the outset and environment-dependent adjustments. Building on the principled theory of online convex optimization, we present a parameter-free optimizer for lifelong RL, called <strong>TRAC</strong>, which requires no tuning or prior knowledge about the distribution shifts. Extensive experiments on Procgen, Atari, and Gym Control environments show that <strong>TRAC</strong> works surprisingly well‚Äîmitigating loss of plasticity and rapidly adapting to challenging distribution shifts‚Äîdespite the underlying optimization problem being nonconvex and nonstationary.</left>
    </p>
    <p>
    </p>

    <p style="background: #fff3b0;">
        <center style="background: #fff3b0;">
            Try <strong>TRAC</strong> in your lifelong or continual experiments with just <strong><a href="https://github.com/ComputationalRobotics/PACE">one line change.&nbsp;</a></strong> <a href="https://github.com/minyoungg/vqtorch"><strong>&nbsp;</strong></a>
        </center>
    </p>

    <br>

    <hr>
    <center>
        <h2> Lifelong RL suffers from <strong>Loss of Plasticity</strong></h2>
    </center>
    <br />

    <p>
        In lifelong RL, a learning agent must continually acquire new knowledge to handle the nonstationarity of the environment. At first glance, there appears to be an obvious solution:&nbsp; &nbsp;given a policy gradient oracle, the agent could just keep running gradient descent nonstop. However, recent experiments have demonstrated an intriguing behavior called&nbsp;<strong>loss of plasticity <a href="https://arxiv.org/abs/2108.06325">[1</a>,<a href="https://arxiv.org/abs/2204.09560">2</a>,<a href="https://arxiv.org/abs/2303.07507">3</a>,<a href="https://arxiv.org/abs/2302.12902">4</a>]:</strong>&nbsp;despite persistent gradient steps, such an agent can gradually lose its responsiveness to incoming observations.</p>
    <br />
    <div style="text-align: center;">
        <img class="rounded" src="./assets/starpilot_rewards_animation.gif" style="width: 350px; display: inline-block;">
        <img class="rounded" src="./assets/control_rewards_animation.gif" style="width: 265px; display: inline-block;">
    </div>
    <br />

    <hr>
    <center>
        <h2> Surprisingly, in this non-convex setting, online <em><strong>convex</strong></em> optimization can help.</h2>
    </center>

    <br />
    <p>
        <strong>TRAC</strong> combines three parameter-free Online Convex Optimization (OCO) techniques:
        <a href="https://arxiv.org/abs/1802.06293" target="_blank">direction-magnitude decomposition</a>,
        <a href="https://arxiv.org/abs/1902.09003" target="_blank">additive aggregation</a>, and the
        <a href="https://arxiv.org/pdf/2402.02720" target="_blank">\(\text{erfi}\) potential function</a>.
        The algorithm starts with a base optimizer, \(\text{Base}\), and adjusts a scaling parameter, \( S_{t+1} \), in an online data-dependent manner.
        This parameter affects the update of \(\theta_{t+1}\) as shown:
    </p>

    \[
    \theta_{t+1} = S_{t+1} \cdot \theta_{t+1}^\text{base} + (1 - S_{t+1}) \theta_\text{ref}.
    \]
    </p>
    <p>The decision rule for the tuner uses the \(\text{erfi}\) function to calculate \( s_{t+1} \) as follows:</p>
    <p>
        \[
    s_{t+1} = \frac{\epsilon}{(\text{erfi})(1/\sqrt{2})} (\text{erfi})\left(\frac{\sigma_t}{\sqrt{2v_t} + \epsilon}\right),
    \]
    </p>
    <p>This rule applies the \(\text{erfi}\) function, an imaginary error function, to tune the scaling parameter based on the input \(\sigma_t\) and the running variance \(v_t\). Aggregating the outputs of tuners with different discount factors allows <strong>TRAC</strong> to adaptively scale based on algorithm performance without manual tuning.</p>

    <br />
    <center>
        <img class="rounded" src="./assets/fastTRAC.gif" width="600px">
    </center>
    <br />
    <hr>
    <center>
        <h2>Experimental Results</h2>
    </center>
    <p><img class="rounded" src="./assets/combined_plots.png" width="100%"></p>
    <br />
    <hr>
    <center>
        <h2> Try <strong>TRAC</strong> in PyTorch with <em>one line </em></h2>
    </center>

    <p>
        <center style="background: #fff3b0;"> For full examples using <b>TRAC</b> with PPO in lifelong RL see <a href="https://github.com/ComputationalRobotics/TRAC/tree/main">here</a>. </center>
    </p>

    <br />
    <center>
        <h3 First Install <strong>Install TRAC</strong> <a href="https://pypi.org/project/trac-optimizer/"> [<b>Pypi</b>] </a> </h3>
        <div class="install">
            <pre><code class="language-shell"><font color="#1a936f">pip install</font> trac-optimizer</code></pre>
        </div>
    </center>


    <br />
    <div class="flex-item">
        <h3 class="code-title">Original</h3>
        <div class="code-container">
            <pre><code class="language-python">
<font color="#ef476f">from</font> torch.optim <font color="#ef476f">import</font> Adam
# original code
optimizer = Adam(model.parameters(), lr=<font color="#F39C12">0.01</font>)
<font color="#7F8C8D"># your typical optimizer methods</font>
optimizer.<font color="#2E86C1">zero_grad</font>() 
optimizer.<font color="#2E86C1">step</font>()
            </code></pre>
        </div>
    </div>
    <div class="flex-item">
        <h3 class="code-title">With TRAC</h3>
        <div class="code-container">
            <pre><code class="language-python">
<font color="#ef476f">from</font> trac_optimizer <font color="#ef476f">import</font> start_trac
# with TRAC
optimizer = start_trac(log_file='logs/trac.text', Adam)(model.parameters(), lr=<font color="#F39C12">0.01</font>)
<font color="#7F8C8D"># using your optimizer methods exactly as you did before (feel free to use others as well)</font>
optimizer.<font color="#2E86C1">zero_grad</font>() 
optimizer.<font color="#2E86C1">step</font>()
            </code></pre>
        </div>
    </div>

    <br>
    <hr>
    <center>
        <h2> Acknowledgements </h2>
    </center>
    <p> We thank <a href="https://ashok.cutkosky.com">Ashok Cutkosky</a> for insightful discussions on online optimization in nonstationary settings. We are grateful to <a href="https://david-abel.github.io">David Abel</a> for his thoughtful insights on loss of plasticity in relation to lifelong reinforcement learning. We appreciate <a href="https://kzhang66.github.io">Kaiqing Zhang</a> and <a href="https://huyangsh.github.io">Yang Hu</a> for their comments on theoretical and nonstationary RL. This project is partially funded by <a href="https://research.fas.harvard.edu/deans-competitive-fund-promising-scholarship#:~:text=This%20program%20is%20open%20to,not%20eligible%20for%20this%20program.">Harvard University Dean's Competitive Fund for Promising Scholarship. </a><br>
        <br>
        <br><br>
    </p>
    <br>

    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-70157890-3"></script>
    <script>
        window.dataLayer = window.dataLayer || [];

        function gtag() {
            dataLayer.push(arguments);
        }
        gtag('js', new Date());

        gtag('config', 'UA-70157890-3');
    </script>

</body>

</html>
